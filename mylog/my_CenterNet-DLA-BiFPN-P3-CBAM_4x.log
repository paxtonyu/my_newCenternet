nohup: 忽略输入
Config './configs/my_CenterNet-DLA-BiFPN-P3-CBAM_4x.yaml' has no VERSION. Assuming it to be compatible with latest v2.
Command Line Args: Namespace(config_file='./configs/my_CenterNet-DLA-BiFPN-P3-CBAM_4x.yaml', dist_url='tcp://127.0.0.1:12162', eval_only=False, machine_rank=1, manual_device='', num_gpus=1, num_machines=1, opts=[], resume=False)
[04/25 15:09:09 detectron2]: Rank of current process: 0. World size: 1
[04/25 15:09:09 detectron2]: Environment info:
-------------------------------  ---------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.16 (default, Mar  2 2023, 03:21:46) [GCC 11.2.0]
numpy                            1.23.5
detectron2                       0.6 @/home/puchyu/projects/detectron2/detectron2
Compiler                         GCC 8.4
CUDA compiler                    CUDA 10.2
detectron2 arch flags            6.1
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.10.0 @/home/puchyu/app/miniconda3/envs/center/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA TITAN Xp (arch=6.1)
Driver version                   470.161.03
CUDA_HOME                        /usr/local/cuda
Pillow                           8.3.2
torchvision                      0.11.0 @/home/puchyu/app/miniconda3/envs/center/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.7.0
-------------------------------  ---------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/25 15:09:09 detectron2]: Command line arguments: Namespace(config_file='./configs/my_CenterNet-DLA-BiFPN-P3-CBAM_4x.yaml', dist_url='tcp://127.0.0.1:12162', eval_only=False, machine_rank=1, manual_device='', num_gpus=1, num_machines=1, opts=[], resume=False)
[04/25 15:09:09 detectron2]: Contents of args.config_file=./configs/my_CenterNet-DLA-BiFPN-P3-CBAM_4x.yaml:
MODEL:
  META_ARCHITECTURE: "CenterNetDetector"
  PROPOSAL_GENERATOR:
    NAME: "CenterNet"
  PIXEL_STD: [57.375, 57.120, 58.395]
  BACKBONE:
    NAME: "my_build_p35_dla_bifpn_cbam_backbone"
  BIFPN:
    OUT_CHANNELS: 160
    NUM_LEVELS: 3
    NUM_BIFPN: 3
  DLA:
    NUM_LAYERS: 34
    NORM: "BN"
  FPN:
    IN_FEATURES: ["dla3", "dla4", "dla5"]
  CENTERNET:
    IN_FEATURES: ['p3', 'p4', 'p5']
    FPN_STRIDES: [8, 16, 32]
    SOI: [[0, 64], [48, 192], [128, 1000000]]
    NUM_CLS_CONVS: 1
    NUM_BOX_CONVS: 1
    REG_WEIGHT: 1.
  WEIGHTS: "./models/ImageNetPretrained/MSRA_R-50.pkl"
DATASETS:
  TRAIN: ("coco_2017_train",)
  TEST: ("coco_2017_val",)
SOLVER:
  IMS_PER_BATCH: 16
  BASE_LR: 0.02
  STEPS: (300000, 340000)
  MAX_ITER: 360000
  CHECKPOINT_PERIOD: 100000
  WARMUP_ITERS: 4000
  WARMUP_FACTOR: 0.00025
  CLIP_GRADIENTS:
    ENABLED: True
INPUT:
  CUSTOM_AUG: EfficientDetResizeCrop
  TRAIN_SIZE: 640
  MIN_SIZE_TEST: 608
  MAX_SIZE_TEST: 900
TEST:
  EVAL_PERIOD: 5000
DATALOADER:
  NUM_WORKERS: 8
OUTPUT_DIR: "./output/MY/auto"
[04/25 15:09:09 detectron2]: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val
  TRAIN:
  - coco_2017_train
DEBUG: false
DEBUG_SHOW_NAME: false
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: false
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  CUSTOM_AUG: EfficientDetResizeCrop
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 900
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 608
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  NOT_CLAMP_BOX: false
  RANDOM_FLIP: horizontal
  SCALE_RANGE:
  - 0.1
  - 2.0
  TEST_INPUT_TYPE: default
  TEST_SIZE: 640
  TRAIN_SIZE: 640
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: my_build_p35_dla_bifpn_cbam_backbone
  BIFPN:
    NORM: GN
    NUM_BIFPN: 3
    NUM_LEVELS: 3
    OUT_CHANNELS: 160
    SEPARABLE_CONV: false
  CENTERNET:
    AS_PROPOSAL: false
    CENTER_NMS: false
    FPN_STRIDES:
    - 8
    - 16
    - 32
    HM_FOCAL_ALPHA: 0.25
    HM_FOCAL_BETA: 4
    HM_MIN_OVERLAP: 0.8
    IGNORE_HIGH_FP: -1.0
    INFERENCE_TH: 0.05
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOC_LOSS_TYPE: giou
    LOSS_GAMMA: 2.0
    MIN_RADIUS: 4
    MORE_POS: false
    MORE_POS_THRESH: 0.2
    MORE_POS_TOPK: 9
    NEG_WEIGHT: 1.0
    NMS_TH_TEST: 0.6
    NMS_TH_TRAIN: 0.6
    NORM: GN
    NOT_NMS: false
    NOT_NORM_REG: true
    NO_REDUCE: false
    NUM_BOX_CONVS: 1
    NUM_CLASSES: 80
    NUM_CLS_CONVS: 1
    NUM_SHARE_CONVS: 0
    ONLY_PROPOSAL: false
    POST_NMS_TOPK_TEST: 100
    POST_NMS_TOPK_TRAIN: 100
    POS_WEIGHT: 1.0
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 1000
    PRIOR_PROB: 0.01
    REG_WEIGHT: 1.0
    SIGMOID_CLAMP: 0.0001
    SOI:
    - - 0
      - 64
    - - 48
      - 192
    - - 128
      - 1000000
    USE_DEFORMABLE: false
    WITH_AGN_HM: false
  DEVICE: cuda
  DLA:
    DLAUP_IN_FEATURES:
    - dla3
    - dla4
    - dla5
    DLAUP_NODE: conv
    MS_OUTPUT: false
    NORM: BN
    NUM_LAYERS: 34
    OUT_FEATURES:
    - dla2
    USE_DLA_UP: true
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES:
    - dla3
    - dla4
    - dla5
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: CenterNetDetector
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 103.53
  - 116.28
  - 123.675
  PIXEL_STD:
  - 57.375
  - 57.12
  - 58.395
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: CenterNet
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CAT_FREQ_PATH: datasets/lvis/lvis_v1_train_cat_info.json
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    EQL_FREQ_CAT: 200
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT: 0.5
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CAT: 50
    FED_LOSS_NUM_CLASSES: 50
    MULT_PROPOSAL_SCORE: false
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    PRIOR_PROB: 0.01
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_EQL_LOSS: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: ./models/ImageNetPretrained/MSRA_R-50.pkl
OUTPUT_DIR: ./output/MY/my_CenterNet-DLA-BiFPN-P3-CBAM_4x
SAVE_DEBUG: false
SAVE_PTH: false
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.02
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 100000
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 360000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  RESET_ITER: false
  STEPS:
  - 300000
  - 340000
  TRAIN_ITER: -1
  WARMUP_FACTOR: 0.00025
  WARMUP_ITERS: 4000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
VIS_THRESH: 0.3

[04/25 15:09:09 detectron2]: Full config saved to ./output/MY/my_CenterNet-DLA-BiFPN-P3-CBAM_4x/config.yaml
[04/25 15:09:10 d2.utils.env]: Using a generated random seed 11143287
Loading pretrained DLA!
[04/25 15:09:13 detectron2]: Model:
CenterNetDetector(
  (backbone): BiFPN_with_CBAM(
    (bottom_up): BackboneWithTopLevels(
      (backbone): DLA(
        (base_layer): Sequential(
          (0): Conv2d(3, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (level0): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (level1): Sequential(
          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (level2): Tree(
          (tree1): BasicBlock(
            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (tree2): BasicBlock(
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (root): Root(
            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (downsample): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (project): Sequential(
            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (level3): Tree(
          (tree1): Tree(
            (tree1): BasicBlock(
              (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (tree2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (root): Root(
              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
            )
            (downsample): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            (project): Sequential(
              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (tree2): Tree(
            (tree1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (tree2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (root): Root(
              (conv): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
            )
          )
          (downsample): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (project): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (level4): Tree(
          (tree1): Tree(
            (tree1): BasicBlock(
              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (tree2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (root): Root(
              (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
            )
            (downsample): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            (project): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (tree2): Tree(
            (tree1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (tree2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (root): Root(
              (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
            )
          )
          (downsample): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (project): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (level5): Tree(
          (tree1): BasicBlock(
            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (tree2): BasicBlock(
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (root): Root(
            (conv): Conv2d(1280, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (downsample): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (project): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (repeated_bifpn): ModuleList(
      (0): SingleBiFPN(
        (lateral_1_f1): Conv2d(
          256, 160, kernel_size=(1, 1), stride=(1, 1)
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (lateral_2_f1): Conv2d(
          512, 160, kernel_size=(1, 1), stride=(1, 1)
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (outputs_f1_1_2): Conv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (lateral_0_f0): Conv2d(
          128, 160, kernel_size=(1, 1), stride=(1, 1)
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (outputs_f0_0_3): Conv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (outputs_f1_1_3_4): Conv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (lateral_2_f2): Conv2d(
          512, 160, kernel_size=(1, 1), stride=(1, 1)
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (outputs_f2_2_5): Conv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
      )
      (1): CBAM(
        (ca): ChannelAttention(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (max_pool): AdaptiveMaxPool2d(output_size=1)
          (shared_MLP): Sequential(
            (0): Conv2d(160, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU()
            (2): Conv2d(10, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (sigmoid): Sigmoid()
        )
        (sa): SpatialAttention(
          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
          (sigmoid): Sigmoid()
        )
      )
      (2): SingleBiFPN(
        (outputs_f1_1_2): Conv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (outputs_f0_0_3): Conv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (outputs_f1_1_3_4): Conv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (outputs_f2_2_5): Conv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
      )
      (3): CBAM(
        (ca): ChannelAttention(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (max_pool): AdaptiveMaxPool2d(output_size=1)
          (shared_MLP): Sequential(
            (0): Conv2d(160, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU()
            (2): Conv2d(10, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (sigmoid): Sigmoid()
        )
        (sa): SpatialAttention(
          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
          (sigmoid): Sigmoid()
        )
      )
      (4): SingleBiFPN(
        (outputs_f1_1_2): Conv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (outputs_f0_0_3): Conv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (outputs_f1_1_3_4): Conv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
        (outputs_f2_2_5): Conv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): GroupNorm(32, 160, eps=1e-05, affine=True)
        )
      )
      (5): CBAM(
        (ca): ChannelAttention(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (max_pool): AdaptiveMaxPool2d(output_size=1)
          (shared_MLP): Sequential(
            (0): Conv2d(160, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU()
            (2): Conv2d(10, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (sigmoid): Sigmoid()
        )
        (sa): SpatialAttention(
          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
          (sigmoid): Sigmoid()
        )
      )
    )
  )
  (proposal_generator): CenterNet(
    (iou_loss): IOULoss()
    (centernet_head): CenterNetHead(
      (cls_tower): Sequential(
        (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): GroupNorm(32, 160, eps=1e-05, affine=True)
        (2): ReLU()
      )
      (bbox_tower): Sequential(
        (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): GroupNorm(32, 160, eps=1e-05, affine=True)
        (2): ReLU()
      )
      (share_tower): Sequential()
      (bbox_pred): Conv2d(160, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (scales): ModuleList(
        (0): Scale()
        (1): Scale()
        (2): Scale()
      )
      (cls_logits): Conv2d(160, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)
[04/25 15:09:13 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./models/ImageNetPretrained/MSRA_R-50.pkl ...
[04/25 15:09:13 fvcore.common.checkpoint]: [Checkpointer] Loading from ./models/ImageNetPretrained/MSRA_R-50.pkl ...
[04/25 15:09:13 d2.checkpoint.c2_model_loading]: Renaming Caffe2 weights ......
WARNING [04/25 15:09:13 d2.checkpoint.c2_model_loading]: No weights in checkpoint matched with model.
WARNING [04/25 15:09:13 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
backbone.bottom_up.backbone.base_layer.0.weight
backbone.bottom_up.backbone.base_layer.1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level0.0.weight
backbone.bottom_up.backbone.level0.1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level1.0.weight
backbone.bottom_up.backbone.level1.1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level2.project.0.weight
backbone.bottom_up.backbone.level2.project.1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level2.root.bn.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level2.root.conv.weight
backbone.bottom_up.backbone.level2.tree1.bn1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level2.tree1.bn2.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level2.tree1.conv1.weight
backbone.bottom_up.backbone.level2.tree1.conv2.weight
backbone.bottom_up.backbone.level2.tree2.bn1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level2.tree2.bn2.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level2.tree2.conv1.weight
backbone.bottom_up.backbone.level2.tree2.conv2.weight
backbone.bottom_up.backbone.level3.project.0.weight
backbone.bottom_up.backbone.level3.project.1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level3.tree1.project.0.weight
backbone.bottom_up.backbone.level3.tree1.project.1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level3.tree1.root.bn.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level3.tree1.root.conv.weight
backbone.bottom_up.backbone.level3.tree1.tree1.bn1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level3.tree1.tree1.bn2.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level3.tree1.tree1.conv1.weight
backbone.bottom_up.backbone.level3.tree1.tree1.conv2.weight
backbone.bottom_up.backbone.level3.tree1.tree2.bn1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level3.tree1.tree2.bn2.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level3.tree1.tree2.conv1.weight
backbone.bottom_up.backbone.level3.tree1.tree2.conv2.weight
backbone.bottom_up.backbone.level3.tree2.root.bn.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level3.tree2.root.conv.weight
backbone.bottom_up.backbone.level3.tree2.tree1.bn1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level3.tree2.tree1.bn2.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level3.tree2.tree1.conv1.weight
backbone.bottom_up.backbone.level3.tree2.tree1.conv2.weight
backbone.bottom_up.backbone.level3.tree2.tree2.bn1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level3.tree2.tree2.bn2.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level3.tree2.tree2.conv1.weight
backbone.bottom_up.backbone.level3.tree2.tree2.conv2.weight
backbone.bottom_up.backbone.level4.project.0.weight
backbone.bottom_up.backbone.level4.project.1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level4.tree1.project.0.weight
backbone.bottom_up.backbone.level4.tree1.project.1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level4.tree1.root.bn.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level4.tree1.root.conv.weight
backbone.bottom_up.backbone.level4.tree1.tree1.bn1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level4.tree1.tree1.bn2.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level4.tree1.tree1.conv1.weight
backbone.bottom_up.backbone.level4.tree1.tree1.conv2.weight
backbone.bottom_up.backbone.level4.tree1.tree2.bn1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level4.tree1.tree2.bn2.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level4.tree1.tree2.conv1.weight
backbone.bottom_up.backbone.level4.tree1.tree2.conv2.weight
backbone.bottom_up.backbone.level4.tree2.root.bn.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level4.tree2.root.conv.weight
backbone.bottom_up.backbone.level4.tree2.tree1.bn1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level4.tree2.tree1.bn2.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level4.tree2.tree1.conv1.weight
backbone.bottom_up.backbone.level4.tree2.tree1.conv2.weight
backbone.bottom_up.backbone.level4.tree2.tree2.bn1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level4.tree2.tree2.bn2.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level4.tree2.tree2.conv1.weight
backbone.bottom_up.backbone.level4.tree2.tree2.conv2.weight
backbone.bottom_up.backbone.level5.project.0.weight
backbone.bottom_up.backbone.level5.project.1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level5.root.bn.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level5.root.conv.weight
backbone.bottom_up.backbone.level5.tree1.bn1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level5.tree1.bn2.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level5.tree1.conv1.weight
backbone.bottom_up.backbone.level5.tree1.conv2.weight
backbone.bottom_up.backbone.level5.tree2.bn1.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level5.tree2.bn2.{bias, running_mean, running_var, weight}
backbone.bottom_up.backbone.level5.tree2.conv1.weight
backbone.bottom_up.backbone.level5.tree2.conv2.weight
backbone.repeated_bifpn.0.lateral_0_f0.norm.{bias, weight}
backbone.repeated_bifpn.0.lateral_0_f0.{bias, weight}
backbone.repeated_bifpn.0.lateral_1_f1.norm.{bias, weight}
backbone.repeated_bifpn.0.lateral_1_f1.{bias, weight}
backbone.repeated_bifpn.0.lateral_2_f1.norm.{bias, weight}
backbone.repeated_bifpn.0.lateral_2_f1.{bias, weight}
backbone.repeated_bifpn.0.lateral_2_f2.norm.{bias, weight}
backbone.repeated_bifpn.0.lateral_2_f2.{bias, weight}
backbone.repeated_bifpn.0.outputs_f0_0_3.norm.{bias, weight}
backbone.repeated_bifpn.0.outputs_f0_0_3.weight
backbone.repeated_bifpn.0.outputs_f1_1_2.norm.{bias, weight}
backbone.repeated_bifpn.0.outputs_f1_1_2.weight
backbone.repeated_bifpn.0.outputs_f1_1_3_4.norm.{bias, weight}
backbone.repeated_bifpn.0.outputs_f1_1_3_4.weight
backbone.repeated_bifpn.0.outputs_f2_2_5.norm.{bias, weight}
backbone.repeated_bifpn.0.outputs_f2_2_5.weight
backbone.repeated_bifpn.0.{weights_f0_0_3, weights_f1_1_2, weights_f1_1_3_4, weights_f2_2_5}
backbone.repeated_bifpn.1.ca.shared_MLP.0.weight
backbone.repeated_bifpn.1.ca.shared_MLP.2.weight
backbone.repeated_bifpn.1.sa.conv1.weight
backbone.repeated_bifpn.2.outputs_f0_0_3.norm.{bias, weight}
backbone.repeated_bifpn.2.outputs_f0_0_3.weight
backbone.repeated_bifpn.2.outputs_f1_1_2.norm.{bias, weight}
backbone.repeated_bifpn.2.outputs_f1_1_2.weight
backbone.repeated_bifpn.2.outputs_f1_1_3_4.norm.{bias, weight}
backbone.repeated_bifpn.2.outputs_f1_1_3_4.weight
backbone.repeated_bifpn.2.outputs_f2_2_5.norm.{bias, weight}
backbone.repeated_bifpn.2.outputs_f2_2_5.weight
backbone.repeated_bifpn.2.{weights_f0_0_3, weights_f1_1_2, weights_f1_1_3_4, weights_f2_2_5}
backbone.repeated_bifpn.3.ca.shared_MLP.0.weight
backbone.repeated_bifpn.3.ca.shared_MLP.2.weight
backbone.repeated_bifpn.3.sa.conv1.weight
backbone.repeated_bifpn.4.outputs_f0_0_3.norm.{bias, weight}
backbone.repeated_bifpn.4.outputs_f0_0_3.weight
backbone.repeated_bifpn.4.outputs_f1_1_2.norm.{bias, weight}
backbone.repeated_bifpn.4.outputs_f1_1_2.weight
backbone.repeated_bifpn.4.outputs_f1_1_3_4.norm.{bias, weight}
backbone.repeated_bifpn.4.outputs_f1_1_3_4.weight
backbone.repeated_bifpn.4.outputs_f2_2_5.norm.{bias, weight}
backbone.repeated_bifpn.4.outputs_f2_2_5.weight
backbone.repeated_bifpn.4.{weights_f0_0_3, weights_f1_1_2, weights_f1_1_3_4, weights_f2_2_5}
backbone.repeated_bifpn.5.ca.shared_MLP.0.weight
backbone.repeated_bifpn.5.ca.shared_MLP.2.weight
backbone.repeated_bifpn.5.sa.conv1.weight
proposal_generator.centernet_head.bbox_pred.{bias, weight}
proposal_generator.centernet_head.bbox_tower.0.{bias, weight}
proposal_generator.centernet_head.bbox_tower.1.{bias, weight}
proposal_generator.centernet_head.cls_logits.{bias, weight}
proposal_generator.centernet_head.cls_tower.0.{bias, weight}
proposal_generator.centernet_head.cls_tower.1.{bias, weight}
proposal_generator.centernet_head.scales.0.scale
proposal_generator.centernet_head.scales.1.scale
proposal_generator.centernet_head.scales.2.scale
WARNING [04/25 15:09:13 fvcore.common.checkpoint]: The checkpoint state_dict contains keys that are not used by the model:
  stem.conv1.{bias, weight}
  fc1000.{bias, weight}
  res2.0.shortcut.norm.{bias, running_mean, running_var, weight}
  res2.0.shortcut.weight
  res2.0.conv1.norm.{bias, running_mean, running_var, weight}
  res2.0.conv1.weight
  res2.0.conv2.norm.{bias, running_mean, running_var, weight}
  res2.0.conv2.weight
  res2.0.conv3.norm.{bias, running_mean, running_var, weight}
  res2.0.conv3.weight
  res2.1.conv1.norm.{bias, running_mean, running_var, weight}
  res2.1.conv1.weight
  res2.1.conv2.norm.{bias, running_mean, running_var, weight}
  res2.1.conv2.weight
  res2.1.conv3.norm.{bias, running_mean, running_var, weight}
  res2.1.conv3.weight
  res2.2.conv1.norm.{bias, running_mean, running_var, weight}
  res2.2.conv1.weight
  res2.2.conv2.norm.{bias, running_mean, running_var, weight}
  res2.2.conv2.weight
  res2.2.conv3.norm.{bias, running_mean, running_var, weight}
  res2.2.conv3.weight
  res3.0.shortcut.norm.{bias, running_mean, running_var, weight}
  res3.0.shortcut.weight
  res3.0.conv1.norm.{bias, running_mean, running_var, weight}
  res3.0.conv1.weight
  res3.0.conv2.norm.{bias, running_mean, running_var, weight}
  res3.0.conv2.weight
  res3.0.conv3.norm.{bias, running_mean, running_var, weight}
  res3.0.conv3.weight
  res3.1.conv1.norm.{bias, running_mean, running_var, weight}
  res3.1.conv1.weight
  res3.1.conv2.norm.{bias, running_mean, running_var, weight}
  res3.1.conv2.weight
  res3.1.conv3.norm.{bias, running_mean, running_var, weight}
  res3.1.conv3.weight
  res3.2.conv1.norm.{bias, running_mean, running_var, weight}
  res3.2.conv1.weight
  res3.2.conv2.norm.{bias, running_mean, running_var, weight}
  res3.2.conv2.weight
  res3.2.conv3.norm.{bias, running_mean, running_var, weight}
  res3.2.conv3.weight
  res3.3.conv1.norm.{bias, running_mean, running_var, weight}
  res3.3.conv1.weight
  res3.3.conv2.norm.{bias, running_mean, running_var, weight}
  res3.3.conv2.weight
  res3.3.conv3.norm.{bias, running_mean, running_var, weight}
  res3.3.conv3.weight
  res4.0.shortcut.norm.{bias, running_mean, running_var, weight}
  res4.0.shortcut.weight
  res4.0.conv1.norm.{bias, running_mean, running_var, weight}
  res4.0.conv1.weight
  res4.0.conv2.norm.{bias, running_mean, running_var, weight}
  res4.0.conv2.weight
  res4.0.conv3.norm.{bias, running_mean, running_var, weight}
  res4.0.conv3.weight
  res4.1.conv1.norm.{bias, running_mean, running_var, weight}
  res4.1.conv1.weight
  res4.1.conv2.norm.{bias, running_mean, running_var, weight}
  res4.1.conv2.weight
  res4.1.conv3.norm.{bias, running_mean, running_var, weight}
  res4.1.conv3.weight
  res4.2.conv1.norm.{bias, running_mean, running_var, weight}
  res4.2.conv1.weight
  res4.2.conv2.norm.{bias, running_mean, running_var, weight}
  res4.2.conv2.weight
  res4.2.conv3.norm.{bias, running_mean, running_var, weight}
  res4.2.conv3.weight
  res4.3.conv1.norm.{bias, running_mean, running_var, weight}
  res4.3.conv1.weight
  res4.3.conv2.norm.{bias, running_mean, running_var, weight}
  res4.3.conv2.weight
  res4.3.conv3.norm.{bias, running_mean, running_var, weight}
  res4.3.conv3.weight
  res4.4.conv1.norm.{bias, running_mean, running_var, weight}
  res4.4.conv1.weight
  res4.4.conv2.norm.{bias, running_mean, running_var, weight}
  res4.4.conv2.weight
  res4.4.conv3.norm.{bias, running_mean, running_var, weight}
  res4.4.conv3.weight
  res4.5.conv1.norm.{bias, running_mean, running_var, weight}
  res4.5.conv1.weight
  res4.5.conv2.norm.{bias, running_mean, running_var, weight}
  res4.5.conv2.weight
  res4.5.conv3.norm.{bias, running_mean, running_var, weight}
  res4.5.conv3.weight
  res5.0.shortcut.norm.{bias, running_mean, running_var, weight}
  res5.0.shortcut.weight
  res5.0.conv1.norm.{bias, running_mean, running_var, weight}
  res5.0.conv1.weight
  res5.0.conv2.norm.{bias, running_mean, running_var, weight}
  res5.0.conv2.weight
  res5.0.conv3.norm.{bias, running_mean, running_var, weight}
  res5.0.conv3.weight
  res5.1.conv1.norm.{bias, running_mean, running_var, weight}
  res5.1.conv1.weight
  res5.1.conv2.norm.{bias, running_mean, running_var, weight}
  res5.1.conv2.weight
  res5.1.conv3.norm.{bias, running_mean, running_var, weight}
  res5.1.conv3.weight
  res5.2.conv1.norm.{bias, running_mean, running_var, weight}
  res5.2.conv1.weight
  res5.2.conv2.norm.{bias, running_mean, running_var, weight}
  res5.2.conv2.weight
  res5.2.conv3.norm.{bias, running_mean, running_var, weight}
  res5.2.conv3.weight
  stem.conv1.norm.{bias, running_mean, running_var, weight}
[04/25 15:09:13 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [<centernet.data.transforms.custom_augmentation_impl.EfficientDetResizeCrop object at 0x7feb7a3c4910>, RandomFlip()]
[04/25 15:09:44 d2.data.datasets.coco]: Loading datasets/coco/annotations/instances_train2017.json takes 30.21 seconds.
[04/25 15:09:45 d2.data.datasets.coco]: Loaded 118287 images in COCO format from datasets/coco/annotations/instances_train2017.json
[04/25 15:09:56 d2.data.build]: Removed 1021 images with no usable annotations. 117266 images left.
[04/25 15:10:00 d2.data.build]: Distribution of instances among all 80 categories:
|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
|     total     | 849949       |              |              |               |              |
[04/25 15:10:00 d2.data.build]: Using training sampler TrainingSampler
[04/25 15:10:00 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/25 15:10:00 d2.data.common]: Serializing 117266 elements to byte tensors and concatenating them all ...
[04/25 15:10:06 d2.data.common]: Serialized dataset takes 451.21 MiB
[04/25 15:10:08 detectron2]: Starting training from iteration 0
[04/25 15:10:45 d2.utils.events]:  eta: 6 days, 21:12:58  iter: 20  total_loss: 2.061  loss_centernet_pos: 1.167  loss_centernet_neg: 0.1287  loss_centernet_loc: 0.7717    time: 1.6880  last_time: 1.6500  data_time: 0.1316  last_data_time: 0.0316   lr: 9.9976e-05  max_mem: 10596M
[04/25 15:11:06 d2.utils.events]:  eta: 6 days, 4:21:47  iter: 40  total_loss: 1.942  loss_centernet_pos: 1.067  loss_centernet_neg: 0.08501  loss_centernet_loc: 0.7605    time: 1.3290  last_time: 0.9073  data_time: 0.0942  last_data_time: 0.1175   lr: 0.00019995  max_mem: 10596M
[04/25 15:11:26 d2.utils.events]:  eta: 3 days, 18:58:22  iter: 60  total_loss: 1.858  loss_centernet_pos: 0.9929  loss_centernet_neg: 0.1025  loss_centernet_loc: 0.7573    time: 1.1872  last_time: 0.9010  data_time: 0.0809  last_data_time: 0.1431   lr: 0.00029993  max_mem: 10596M
[04/25 15:11:46 d2.utils.events]:  eta: 3 days, 19:05:40  iter: 80  total_loss: 1.816  loss_centernet_pos: 0.9884  loss_centernet_neg: 0.09766  loss_centernet_loc: 0.7225    time: 1.1186  last_time: 0.9201  data_time: 0.0848  last_data_time: 0.0989   lr: 0.0003999  max_mem: 10596M
[04/25 15:12:06 d2.utils.events]:  eta: 3 days, 19:09:43  iter: 100  total_loss: 1.752  loss_centernet_pos: 0.9675  loss_centernet_neg: 0.0967  loss_centernet_loc: 0.667    time: 1.0793  last_time: 0.9074  data_time: 0.0934  last_data_time: 0.1122   lr: 0.00049988  max_mem: 10596M
[04/25 15:12:26 d2.utils.events]:  eta: 3 days, 19:21:35  iter: 120  total_loss: 1.705  loss_centernet_pos: 0.9383  loss_centernet_neg: 0.1059  loss_centernet_loc: 0.6471    time: 1.0546  last_time: 0.9539  data_time: 0.0779  last_data_time: 0.0150   lr: 0.00059985  max_mem: 10596M
[04/25 15:12:47 d2.utils.events]:  eta: 3 days, 19:36:34  iter: 140  total_loss: 1.655  loss_centernet_pos: 0.8986  loss_centernet_neg: 0.1057  loss_centernet_loc: 0.6321    time: 1.0377  last_time: 0.9529  data_time: 0.0880  last_data_time: 0.0783   lr: 0.00069983  max_mem: 10596M
[04/25 15:13:07 d2.utils.events]:  eta: 3 days, 19:54:53  iter: 160  total_loss: 1.638  loss_centernet_pos: 0.9108  loss_centernet_neg: 0.1065  loss_centernet_loc: 0.635    time: 1.0261  last_time: 0.9801  data_time: 0.0918  last_data_time: 0.1168   lr: 0.0007998  max_mem: 10596M
[04/25 15:13:28 d2.utils.events]:  eta: 3 days, 20:11:39  iter: 180  total_loss: 1.611  loss_centernet_pos: 0.8849  loss_centernet_neg: 0.09666  loss_centernet_loc: 0.6236    time: 1.0171  last_time: 0.9780  data_time: 0.0865  last_data_time: 0.0772   lr: 0.00089978  max_mem: 10596M
